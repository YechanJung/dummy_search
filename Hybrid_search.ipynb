{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gzip, json\n",
    "\n",
    "def locale_filter(data, locale='en_US'):\n",
    "    return [data['value'] for data in data if data.get('language_tag') == locale]\n",
    "def file_iterator(file_path):\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "must_have_keys = set(['item_id', 'brand', 'item_name', 'item_keywords', 'bullet_point'])\n",
    "key_filter =  must_have_keys.union(set('product_description'))\n",
    "\n",
    "def get_product_item(product, key_filter=key_filter, locale='en_US'):\n",
    "    item = {}\n",
    "    for key in key_filter:\n",
    "        if key == 'item_id':\n",
    "            item[key] = product[key]\n",
    "        else:\n",
    "            values = locale_filter(product.get(key, []), locale)\n",
    "            if values:\n",
    "                item[key] = ' '.join(values).strip()\n",
    "    return item\n",
    "\n",
    "def json_gz2product(file_path):\n",
    "    data = []\n",
    "    products = file_iterator(file_path)\n",
    "    for product in products:\n",
    "        # at least 'item_id', 'brand', 'item_name', 'item_keywords', 'bullet_point'\n",
    "        item = get_product_item(product)\n",
    "        if item.keys() == must_have_keys:\n",
    "            data.append(item)\n",
    "    return data\n",
    "\n",
    "data = []\n",
    "all_files = os.listdir(\"abo-listings/listings/metadata\")\n",
    "for file in all_files:\n",
    "    file_path = 'abo-listings/listings/metadata/' + file\n",
    "    products = json_gz2product(file_path)\n",
    "    data.extend(products)\n",
    "\n",
    "sentences = []\n",
    "for item in data:\n",
    "    sentences.append(' '.join([item.get('item_name', ''), item.get('bullet_point', ''), item.get('product_description', '')]).strip())\n",
    "total_samples = 19900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Elasticsearch for full-text search\n",
    "# preprocess 안됨\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "es_client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"password\"),\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False\n",
    ")\n",
    "\n",
    "index = 'product'\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"item_id\": {\"type\": \"text\"},\n",
    "            \"brand\": {\"type\": \"text\"},\n",
    "            \"item_name\": {\"type\": \"text\"},\n",
    "            \"item_keywords\": {\"type\": \"text\"},\n",
    "            \"bullet_point\": {\"type\": \"text\"},\n",
    "            \"product_description\": {\"type\": \"text\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es_client.indices.create(index=index, body=mapping)\n",
    "\n",
    "documents = []\n",
    "data_sample = data[:total_samples]\n",
    "for i in range(total_samples):\n",
    "    data = data_sample[i]\n",
    "    documents.append({\n",
    "        '_index': index,\n",
    "        '_id': i+1,\n",
    "        '_source': {\n",
    "            'item_id': data.get('item_id', ''),\n",
    "            'brand': data.get('brand', ''),\n",
    "            'item_name': data.get('item_name', ''),\n",
    "            'item_keywords': data.get('item_keywords', ''),\n",
    "            'bullet_point': data.get('bullet_point', ''),\n",
    "            'product_description': data.get('product_description', ''),\n",
    "        }\n",
    "    })\n",
    "\n",
    "success, _ = bulk(es_client, documents)\n",
    "print(f\"Successfully indexed {success} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Create Index\n",
    "index_name = \"product\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "pc_client = pc.Index(index_name)\n",
    "\n",
    "embeddings =  embedding_model.encode(sentences)  \n",
    "\n",
    "vectors = []\n",
    "for d, e in zip(data, embeddings):\n",
    "    vectors.append({\n",
    "        \"id\": d['item_id'],\n",
    "        \"values\": e,\n",
    "        \"metadata\": {'item_name': d['item_name']},\n",
    "    })\n",
    "\n",
    "\n",
    "window_size = 100\n",
    "total_samples = 19900\n",
    "vector_samples = vectors[:total_samples]\n",
    "\n",
    "# index vectors to Pinecone\n",
    "for i in range(total_samples - window_size + 1):\n",
    "    window = vector_samples[i: i + window_size]\n",
    "    pc_client.upsert(\n",
    "        vectors=window,\n",
    "        namespace=\"ns1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def text_search(query):\n",
    "    q = {\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": query,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    result = es_client.search(index=index, body=q)\n",
    "    hits = result['hits']\n",
    "    return [hit['_source'] for hit in hits['hits']]\n",
    "\n",
    "def vector_search(query, topk=3):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "\n",
    "    results = pc_client.query(\n",
    "        namespace=\"ns1\",\n",
    "        vector=query_embedding,\n",
    "        top_k=topk,\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    sorted_matches = sorted(results['matches'], key=lambda x: x['score'], reverse=True)\n",
    "    return sorted_matches\n",
    "\n",
    "def reciprocal_rank_fusion(results, K=60):\n",
    "    docs = []\n",
    "    rrf_score = []\n",
    "    \n",
    "    for ranked_doc in results:\n",
    "        for rank, doc in enumerate(ranked_doc, 1):\n",
    "            docs.append(doc)\n",
    "            rrf_score.append(1.0 / (rank + K))\n",
    "\n",
    "    scored_docs = zip(docs, rrf_score)\n",
    "    sorted_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_docs\n",
    "\n",
    "def hybrid_search(query, topk=3):\n",
    "    results = [vector_search(query, topk), text_search(query)]\n",
    "    ranked_results = reciprocal_rank_fusion(results)\n",
    "    return ranked_results\n",
    "\n",
    "start_time = time.time()\n",
    "results = hybrid_search(\"Find me a kitchen table\")\n",
    "print(f\"Done in {time.time() - start_time} seconds\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from elasticsearch import AsyncElasticsearch  # Need to use async elasticsearch client\n",
    "\n",
    "async def async_text_search(query):\n",
    "    q = {\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": query,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    result = await es_client.search(index=index, body=q)\n",
    "    hits = result['hits']\n",
    "    return [hit['_source'] for hit in hits['hits']]\n",
    "\n",
    "async def async_vector_search(query, topk=3):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = await pc_client.query(  # Need async Pinecone client\n",
    "        namespace=\"ns1\",\n",
    "        vector=query_embedding,\n",
    "        top_k=topk,\n",
    "        include_values=False,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    return sorted(results['matches'], key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "async def hybrid_search(query, topk=3):\n",
    "    # Run both searches concurrently\n",
    "    vector_results, text_results = await asyncio.gather(\n",
    "        async_vector_search(query, topk),\n",
    "        async_text_search(query)\n",
    "    )\n",
    "    results = [vector_results, text_results]\n",
    "    return reciprocal_rank_fusion(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagLLMReranker\n",
    "reranker = FlagLLMReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n",
    "\n",
    "def get_product_str(product):\n",
    "    name = product['item_name']\n",
    "    bullet_points = product['bullet_point']\n",
    "    descriptions = product['product_description']\n",
    "    text = []\n",
    "    if name:\n",
    "        text.append(\"Product Name: %s\" % name)\n",
    "        if bullet_points:\n",
    "            text.append(\"- bullet points: %s\" % ','.join(bullet_points))\n",
    "        if descriptions:\n",
    "            text.append(\"- description: %s\" % ','.join(descriptions))\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def llm_reranker(query, docs):\n",
    "    pairs = [(query, get_product_str(doc)) for doc in docs]\n",
    "    scores = reranker.compute_score(pairs)\n",
    "    scored_docs = zip(docs, scores)\n",
    "    sorted_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_docs\n",
    "\n",
    "def fetch_doc(results):\n",
    "    ids = []\n",
    "    for result in results[0]:\n",
    "        ids.append(result['id'])\n",
    "    for result in results[1]:\n",
    "        ids.append(result['item_id'])\n",
    "    q = {\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": \" OR \".join(ids),\n",
    "                \"default_field\": \"item_id\",\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    result = es_client.search(index=index, body=q)\n",
    "    hits = result['hits']\n",
    "    return [hit['_source'] for hit in hits['hits']]\n",
    "\n",
    "def hybrid_search2(query, topk=3):\n",
    "    results = [vector_search(query, topk), text_search(query)]\n",
    "    docs = fetch_doc(results)\n",
    "    ranked_results = llm_reranker(query, docs)\n",
    "    return ranked_results\n",
    "\n",
    "start_time = time.time()\n",
    "results = hybrid_search2(\"Find me a kitchen table\")\n",
    "print(f\"Done in {time.time() - start_time} seconds\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# async version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def hybrid_search2(query, topk=3):\n",
    "\n",
    "    vector_results, text_results = await asyncio.gather(\n",
    "        async_vector_search(query, topk),\n",
    "        async_text_search(query)\n",
    "    )\n",
    "    results = [vector_results, text_results]\n",
    "    docs = fetch_doc(results)\n",
    "    ranked_results = llm_reranker(query, docs)\n",
    "    return ranked_results\n",
    "\n",
    "start_time = time.time()\n",
    "results = hybrid_search2(\"Find me a kitchen table\")\n",
    "print(f\"Done in {time.time() - start_time} seconds\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
